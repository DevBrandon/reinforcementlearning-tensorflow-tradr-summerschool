{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# OpenAI Universe part 3: playing Space Invadors with deep reinforcement learning. \n",
    "In part 2 we explored deep q-networks. We implemented a simple network that, if everything went well, was able to solve the Cartpole environment. Atari games are more fun than the CartPole environment, but are also harder to solve. This session is dedicated to playing Atari with deep reinforcement learning. \n",
    "\n",
    "A first warning before you are disappointed is that playing Atari games is more difficult than cartpole, and training times are way longer. This is the reason we toyed around with CartPole in the previous session. \n",
    "\n",
    "In this session I will show how you can use OpenAI gym to replicate the paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). A video of a Breakout playing robot [can be found on Youtube](https://www.youtube.com/watch?v=V1eYniJ0Rnk), as well as a video of a [Enduro playing robot](https://www.youtube.com/watch?v=6kO4eZWeKOM). Demis Hassabis, the CEO of DeepMind, can explain what happend in their experiments in a [very entertaining way](https://youtu.be/rbsqaJwpu6A?t=9m55s). \n",
    "\n",
    "A big difference between the CartPole and Atari task is that the Atari environment gives you the raw pixels as observation. Instead of 4 variables you are now dealing with $210 * 160 * 3 = 100.800$ variables as input. The network you build in part 2 is not going to play very well. This means you can either improve your network yourself, or you can replicate the DeepMind layout. This session is only dedicated to showing what the DeepMind network is able to do. \n",
    "\n",
    "Flood Sung was able to put the network in Tensorflow and [put the code on GitHub](https://github.com/songrotek/DQN-Atari-Tensorflow). I downloaded his network architecture, updated it to the latest Tensorflow version, changed some parameters and added it to the Git repository of this [summerschool session](https://github.com/rmeertens/reinforcementlearning-tensorflow-tradr-summerschool). \n",
    "\n",
    "This tutorial has dependencies on Tensorflow, OpenCV, OpenAI Gym, and some other things. Just as with part 1 and 2 the best thing to do is run this code using [Docker](https://docs.docker.com/engine/installation/#supported-platforms). Run the following command to download my prepared docker image and navigate to http://localhost:8888 to view your Jupyter notebook. \n",
    "```\n",
    "docker run -p 8888:8888 -v rmeertens/tensorflowgym\n",
    "```\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-04 14:15:03,812] Making new env: SpaceInvaders-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: 3136\n",
      "dimension: 3136\n",
      "Successfully loaded: ./savedweights/network-dqn-230000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "import cv2\n",
    "import sys\n",
    "from BrainDQN_Nature import *\n",
    "import numpy as np \n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "actions = env.action_space.n\n",
    "brain = BrainDQN(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Image preprocessing\n",
    "As mentioned above we are dealing with $210 * 160 * 3 = 100.800$ variables. The authors of the Playing Atari with DRL solve this by turning the image to grayscale, resizing to 84 x 110, and removing the first 26 rows as they only contain the score. This gives you $84 * 84 = 7.056$ variables per image.\n",
    "\n",
    "Unfortunately, you need to have a sense of time for some Atari games. For example, what is happening in this image? Is the ball going up? Going down? Left or right? That's why we concatenate the last four \"images\" of 84x84 to get an 84x84x4 image as input (which is $84*84*4=28.224$ input variables for our neural network. \n",
    "![Breakout](https://cbssanfran.files.wordpress.com/2015/02/block-up.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing: (210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3f0cda3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing: (84, 84, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhv4fitKIw154HlEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd472605d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(observation):\n",
    "    observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n",
    "    observation = observation[26:110,:]\n",
    "    ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n",
    "    return np.reshape(observation,(84,84,1))\n",
    "\n",
    "\n",
    "action0 = 0  # do nothing\n",
    "observation0, reward0, terminal, info = env.step(action0)\n",
    "print(\"Before processing: \" + str(np.array(observation0).shape))\n",
    "plt.imshow(np.array(observation0))\n",
    "plt.show()\n",
    "observation0 = preprocess(observation0)\n",
    "print(\"After processing: \" + str(np.array(observation0).shape))\n",
    "plt.imshow(np.array(np.squeeze(observation0)))\n",
    "plt.show()\n",
    "\n",
    "brain.setInitState(observation0)\n",
    "brain.currentState = np.squeeze(brain.currentState)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Network layout\n",
    "\n",
    "Open the file BrainDQN_Nature.py and take a look at the function createQNetwork. You will see that this network consists of: \n",
    "- 3 convolational layers\n",
    "- 2 fully connected layers\n",
    "\n",
    "The convolutional layers might be new to you. The best way to learn about them is by taking a look at the [Udacity course \"Deep Learning\"](https://www.udacity.com/course/deep-learning--ud730), or if you quickly want to know what a conv layer is, [watch this video](https://www.youtube.com/watch?v=jajksuQW4mc). \n",
    "\n",
    "Also note that this implementation uses a target network (discussed in part 2) that is regularly updated. \n",
    "\n",
    "### Learning\n",
    "Most interesting things happen in the BrainDQN_Nature.py script. We ask the brain for an action, process the new observation, and give this back to the brain. This means we only have to use a few lines to start the learning of the network. \n",
    "\n",
    "Note that learning can take a very long time. This script is set to run forever, so start it in the evening and see what the network learned in the morning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEP 0 / STATE observe / EPSILON 1.0\n",
      "TIMESTEP 10000 / STATE observe / EPSILON 1.0\n",
      "TIMESTEP 20000 / STATE observe / EPSILON 1.0\n",
      "TIMESTEP 30000 / STATE observe / EPSILON 1.0\n",
      "TIMESTEP 40000 / STATE observe / EPSILON 1.0\n",
      "TIMESTEP 50000 / STATE observe / EPSILON 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bdd26fa6172b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnextObservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnextObservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextObservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetPerception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextObservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/notebooks/workspace/BrainDQN_Nature.pyc\u001b[0m in \u001b[0;36msetPerception\u001b[0;34m(self, nextObservation, action, reward, terminal)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeStep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mOBSERVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                         \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;31m# print info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/workspace/BrainDQN_Nature.pyc\u001b[0m in \u001b[0;36mtrainQNetwork\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myInput\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactionInput\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateInput\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \t\t\t})\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3830\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3832\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    action = brain.getAction()\n",
    "    actionmax = np.argmax(np.array(action))\n",
    "    \n",
    "    nextObservation,reward,terminal, info = env.step(actionmax)\n",
    "    \n",
    "    if terminal:\n",
    "        nextObservation = env.reset()\n",
    "    nextObservation = preprocess(nextObservation)\n",
    "    brain.setPerception(nextObservation,action,reward,terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "After you let your network train for some hours, interrupt the python kernel and run the following script. \n",
    "It is important to set the epsilon value of the brain to a low value (0.0 or 0.1), otherwise your brain might keep performing random actions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "    \n",
    "frameshistory = []\n",
    "backupepsilon = brain.epsilon\n",
    "\n",
    "brain.epsilon = 0.0\n",
    "\n",
    "for _ in range(450):\n",
    "    action = brain.getAction()\n",
    "    \n",
    "    #print(action)\n",
    "    actionmax = np.argmax(np.array(action))\n",
    "    \n",
    "    nextObservation,reward,terminal, info = env.step(actionmax)\n",
    "    if terminal:\n",
    "        nextObservation = env.reset()\n",
    "    frameshistory.append(nextObservation)\n",
    "    nextObservation = preprocess(nextObservation)\n",
    "    brain.setPerception(nextObservation,action,reward,terminal)\n",
    "brain.epsilon = backupepsilon\n",
    "    \n",
    "display_frames_as_gif(frameshistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercises\n",
    "This session you were handed the network layout and training methods described in the paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). \n",
    "\n",
    "### Team up\n",
    "Humans are very good at learning to play these Atari games. Once I learned aliens kill me with bullets and I got points for killing each alien I was quickly getting many points in the game. Unfortunately our programmed brain is terrible at learning how to play the game (it needs a day, and a lot of frames, before it is able to consistently avoid bullents and kill aliens). Part of the problem is that it is difficult to estimate a reward from observations of random behaviour. The agent would be able to make a better guess of the Q-value if it would be fed with a well-played game of space invaders. The exercises are thus: \n",
    "- Record yourself or a friend playing a game of Space Invaders and save the observations, actions, and rewards in a replay memory (save this memory for later use). Use this game as initial replay memory of the agent. \n",
    "- Record a fully trained agent playing a game of space Invaders and save the observations, actions, and rewards in a replay memory (save this memory for later use). Use this game as initial replay memory of a new agent you are going to train. \n",
    "Think of ways to evaluate how well these agents are doing compared to agents initialised on \"random\" experiences and see what agents are better. \n",
    "### Transfer knowledge\n",
    "Humans are very good at transferring knowledge from one domain to another. Unfortunately, our agent is not that good at this. \n",
    "- Try training and agent on one game, and try to see how long it takes for this agent to learn to play another domain. If this topic interests you, take a look at how Deepmind improved [their agent](https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments \n",
    "This blogpost is the first part of my TRADR summerschool workshop on using human input in reinforcement learning algorithms. More information can be found [on their homepage](https://sites.google.com/view/tradr/home)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
