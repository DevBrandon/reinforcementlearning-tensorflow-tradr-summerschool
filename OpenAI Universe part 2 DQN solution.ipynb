{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# OpenAI Universe part 2: deep q-networks\n",
    "Last part we used a random search algorithm to \"solve\" the cartpole environment. This time we are going to take things to the next level and implement a deep q-network.\n",
    "\n",
    "## Background\n",
    "Q-learning is a reinforcement learning technique that tries to predict the reward of a state-action pair. For the cartpole environment the state consists of four values, and there are two possible actions. For a certain state S we can predict the reward if we were to push left $Q(S,left)$ or right $Q(S,right)$. \n",
    "\n",
    "In the Atari game environment you get a reward of 1 every time you score a point. This scoring can happen when you hit a block in breakout, an alien in Space Invaders, or eat a pallet in Pacman. In the cartpole environment you get a reward every time the pole is standing on the cart (which is: every frame). The trick of q-learning is that it not only considers the direct reward, but also the expected future reward. After applying action $a$ we enter state $S_{t+1}$ and take the following into account: \n",
    "- The reward $r$ we obtained by performing this action\n",
    "- The expected maximum reward $Q(S_{t+1},a)$, in the cartpole environment this is $max(Q(S_{t+1},left), Q(S_{t+1},right)$\n",
    "\n",
    "We combine this into a neat formula where say that the predicted value should be $r$ in a \n",
    "\n",
    "\\begin{equation*}\n",
    "Q(S,a) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "r & \\text{for terminal} S_{t+1} \\\\\n",
    "r + \\gamma max_a Q(S_{t+1},a)& \\text{for nonterminal } S_{t+1}\n",
    "\\end{array} \\right.\n",
    "\\end{equation*}\n",
    "Where $\\gamma$ is the discount factor. Taking a small $\\gamma$ (for example 0.2) means that you don't really care about long-term rewards, a large $\\gamma$ (0.95) means that you care a lot about the long-term rewards. In our case we do care a lot about long-term rewards, so we take a large $\\gamma$. \n",
    "\n",
    "Let's apply our knowledge of q-learning on the same environment we tried last time: the CartPole environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:50:28,161] Making new env: CartPole-v0\n",
      "[2017-07-03 12:50:28,169] Creating monitor directory /tmp/cartpole-experiment-1\n",
      "[2017-07-03 12:50:28,170] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000000.mp4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "from time import gmtime, strftime\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1')\n",
    "observation = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Value approximation\n",
    "There are many ways in which you can estimate the Q-value for each (state,action) pair. The latest \"cool\" thing to do is estimate it using a neural network. This is also what we will be doing!\n",
    "\n",
    "We will build our network in Tensorflow: an open-source libary for machine-learning. If you are not familiar with Tensorflow, the most important thing to know is that we will fist build our network, then initialise it and use it. All python variables are \"placeholders\" in a session. You can find more information on the [Tensorflow homepage](https://www.tensorflow.org/get_started/)\n",
    "\n",
    "I created a very simple network layout with four inputs (the four variables we observe) and two outputs (either push left or right). I added four fully connected layers: \n",
    "- From 4 to 16 variables\n",
    "- From 16 to 32 variables\n",
    "- From 32 to 8 variables\n",
    "- From 8 to 2 variables\n",
    "\n",
    "Every layer is a dense layer with a RELU nonlinearity except for the last layer as this one has to predict the expected Q-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Network input\n",
    "networkstate = tf.placeholder(tf.float32, [None, 4], name=\"input\")\n",
    "networkaction = tf.placeholder(tf.int32, [None], name=\"actioninput\")\n",
    "networkreward = tf.placeholder(tf.float32,[None], name=\"groundtruth_reward\")\n",
    "action_onehot = tf.one_hot(networkaction, 2, name=\"actiononehot\")\n",
    "\n",
    "# The variable in our network: \n",
    "w1 = tf.Variable(tf.random_normal([4,16], stddev=0.35), name=\"W1\")\n",
    "w2 = tf.Variable(tf.random_normal([16,32], stddev=0.35), name=\"W2\")\n",
    "w3 = tf.Variable(tf.random_normal([32,8], stddev=0.35), name=\"W3\")\n",
    "w4 = tf.Variable(tf.random_normal([8,2], stddev=0.35), name=\"W4\")\n",
    "b1 = tf.Variable(tf.zeros([16]), name=\"B1\")\n",
    "b2 = tf.Variable(tf.zeros([32]), name=\"B2\")\n",
    "b3 = tf.Variable(tf.zeros([8]), name=\"B3\")\n",
    "b4 = tf.Variable(tf.zeros(2), name=\"B4\")\n",
    "\n",
    "# The network layout\n",
    "layer1 = tf.nn.relu(tf.add(tf.matmul(networkstate,w1), b1), name=\"Result1\")\n",
    "layer2 = tf.nn.relu(tf.add(tf.matmul(layer1,w2), b2), name=\"Result2\")\n",
    "layer3 = tf.nn.relu(tf.add(tf.matmul(layer2,w3), b3), name=\"Result3\")\n",
    "predictedreward = tf.add(tf.matmul(layer3,w4), b4, name=\"predictedReward\")\n",
    "\n",
    "# Learning \n",
    "qreward = tf.reduce_sum(tf.multiply(predictedreward, action_onehot), reduction_indices = 1)\n",
    "loss = tf.reduce_mean(tf.square(networkreward - qreward))\n",
    "tf.summary.scalar('loss', loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.0001).minimize(loss)\n",
    "merged_summary = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Session management and Tensorboard\n",
    "\n",
    "Now we start the session. I added support for Tensorboard: a nice tool to visualise your learning. At the moment I only added one summary: the loss of the network. \n",
    "If you did not install Docker yet, make sure [you do this](https://docs.docker.com/engine/installation/#supported-platforms). To run tensorboard you have to run:\n",
    "\n",
    "```\n",
    "docker run -p 6006:6006 -v $(pwd):/mounted rmeertens/tensorboard\n",
    "```\n",
    "\n",
    "Then navigate to localhost:6006 to see your tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "summary_writer = tf.summary.FileWriter('trainsummary',sess.graph)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Learning Q(S,a)\n",
    "An interesting paper you can use as guideline for deep q-networks is \"Playing Atari with Deep Reinforcement Learning (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). This paper by deepmind explains how they were able to teach a neural network to play Atari games. \n",
    "\n",
    "One of the main contributions of this paper is their use of an \"experience replay mechanism\". If you were to train your neural network in the order of images you see normally the network quickly forgets what it saw before. To fix this we save what we saw in a memory with the following variables: \n",
    "\n",
    "($S$, $action$, $reward$, $is terminal$, $S_{t+1}$)\n",
    "\n",
    "Now every frame we sample a random minibatch of our memory and train our network on that. We also only keep the newer experiences to keep our memory fresh with good actions. The full algorithm in their paper looks like this: \n",
    "![dqn algorith](dqn alg.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:50:28,764] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000001.mp4\n",
      "[2017-07-03 12:50:28,884] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000008.mp4\n",
      "[2017-07-03 12:50:29,016] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000027.mp4\n",
      "[2017-07-03 12:50:29,193] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001, loss: 0.878417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:50:29,586] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000125.mp4\n",
      "[2017-07-03 12:50:30,450] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2001, loss: 2.612569\n",
      "Epoch 3001, loss: 2.125304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:50:31,621] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4001, loss: 0.685737\n",
      "Epoch 5001, loss: 0.848861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:50:33,332] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6001, loss: 4.561125\n",
      "Epoch 7001, loss: 1.534148\n",
      "Epoch 8001, loss: 0.372116\n",
      "Epoch 9001, loss: 0.539587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:50:36,870] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10001, loss: 0.227657\n",
      "Epoch 11001, loss: 0.071213\n",
      "Epoch 12001, loss: 0.081353\n",
      "Epoch 13001, loss: 3.078383\n",
      "Epoch 14001, loss: 0.184846\n",
      "Epoch 15001, loss: 0.426586\n",
      "Epoch 16001, loss: 0.040447\n",
      "Epoch 17001, loss: 0.969385\n",
      "Epoch 18001, loss: 0.104043\n",
      "Epoch 19001, loss: 1.657083\n",
      "Epoch 20001, loss: 0.097585\n",
      "Epoch 21001, loss: 0.051327\n",
      "Epoch 22001, loss: 0.182340\n",
      "Epoch 23001, loss: 0.028440\n",
      "Epoch 24001, loss: 0.038255\n",
      "Epoch 25001, loss: 0.084144\n",
      "Epoch 26001, loss: 0.156991\n",
      "Epoch 27001, loss: 0.009564\n",
      "Epoch 28001, loss: 0.012276\n",
      "Epoch 29001, loss: 0.022192\n",
      "Epoch 30001, loss: 0.072836\n",
      "Epoch 31001, loss: 0.036808\n",
      "Epoch 32001, loss: 1.623818\n",
      "Epoch 33001, loss: 0.045964\n",
      "Epoch 34001, loss: 0.124939\n",
      "Epoch 35001, loss: 0.027984\n",
      "Epoch 36001, loss: 0.092817\n",
      "Epoch 37001, loss: 0.024530\n",
      "Epoch 38001, loss: 0.014200\n",
      "Epoch 39001, loss: 0.021270\n",
      "Epoch 40001, loss: 0.022633\n",
      "Epoch 41001, loss: 0.021919\n",
      "Epoch 42001, loss: 1.377276\n",
      "Epoch 43001, loss: 0.021066\n",
      "Epoch 44001, loss: 0.023433\n",
      "Epoch 45001, loss: 0.030719\n",
      "Epoch 46001, loss: 0.010759\n",
      "Epoch 47001, loss: 1.392302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:51:12,883] Starting new video recorder writing to /tmp/cartpole-experiment-1/openaigym.video.0.26236.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48001, loss: 0.017084\n",
      "Epoch 49001, loss: 0.032988\n",
      "Epoch 50001, loss: 0.017523\n",
      "Epoch 51001, loss: 0.020461\n",
      "Epoch 52001, loss: 0.033347\n",
      "Epoch 53001, loss: 0.053939\n",
      "Epoch 54001, loss: 0.029761\n",
      "Epoch 55001, loss: 0.038916\n",
      "Epoch 56001, loss: 0.026225\n",
      "Epoch 57001, loss: 0.017586\n",
      "Epoch 58001, loss: 0.014760\n",
      "Epoch 59001, loss: 0.015527\n",
      "Epoch 60001, loss: 0.013314\n",
      "Epoch 61001, loss: 0.012487\n",
      "Epoch 62001, loss: 0.020720\n",
      "Epoch 63001, loss: 0.008561\n",
      "Epoch 64001, loss: 0.009799\n",
      "Epoch 65001, loss: 0.007222\n",
      "Epoch 66001, loss: 0.010561\n",
      "Epoch 67001, loss: 0.007533\n",
      "Epoch 68001, loss: 0.007155\n",
      "Epoch 69001, loss: 0.623195\n",
      "Epoch 70001, loss: 0.517900\n",
      "Epoch 71001, loss: 0.007171\n",
      "Epoch 72001, loss: 0.003998\n",
      "Epoch 73001, loss: 0.003392\n",
      "Epoch 74001, loss: 0.005929\n",
      "Epoch 75001, loss: 0.003257\n",
      "Epoch 76001, loss: 0.006366\n",
      "Epoch 77001, loss: 0.006322\n",
      "Epoch 78001, loss: 0.917649\n",
      "Epoch 79001, loss: 0.013265\n",
      "Epoch 80001, loss: 0.006011\n",
      "Epoch 81001, loss: 0.507124\n",
      "Epoch 82001, loss: 0.408160\n",
      "Epoch 83001, loss: 0.554875\n",
      "Epoch 84001, loss: 0.001334\n",
      "Epoch 85001, loss: 0.007422\n",
      "Epoch 86001, loss: 0.002750\n",
      "Epoch 87001, loss: 0.229926\n",
      "Epoch 88001, loss: 0.005623\n",
      "Epoch 89001, loss: 0.001632\n",
      "Epoch 90001, loss: 0.065727\n",
      "Epoch 91001, loss: 0.002007\n",
      "Epoch 92001, loss: 0.006548\n",
      "Epoch 93001, loss: 0.005149\n",
      "Epoch 94001, loss: 0.010786\n",
      "Epoch 95001, loss: 0.015996\n",
      "Epoch 96001, loss: 0.002318\n",
      "Epoch 97001, loss: 0.004908\n",
      "Epoch 98001, loss: 0.004474\n",
      "Epoch 99001, loss: 0.011559\n",
      "Epoch 100001, loss: 0.007090\n",
      "Epoch 101001, loss: 0.006924\n",
      "Epoch 102001, loss: 0.006863\n",
      "Epoch 103001, loss: 0.003666\n",
      "Epoch 104001, loss: 0.009046\n",
      "Epoch 105001, loss: 0.008076\n",
      "Epoch 106001, loss: 0.023165\n",
      "Epoch 107001, loss: 0.006484\n",
      "Epoch 108001, loss: 0.019643\n",
      "Epoch 109001, loss: 0.004879\n",
      "Epoch 110001, loss: 0.017488\n",
      "Epoch 111001, loss: 1.572641\n",
      "Epoch 112001, loss: 0.010865\n",
      "Epoch 113001, loss: 0.009298\n",
      "Epoch 114001, loss: 0.009628\n",
      "Epoch 115001, loss: 0.003265\n",
      "Epoch 116001, loss: 0.003882\n",
      "Epoch 117001, loss: 2.967951\n",
      "Epoch 118001, loss: 0.014943\n",
      "Epoch 119001, loss: 0.007879\n",
      "Epoch 120001, loss: 0.014652\n",
      "Epoch 121001, loss: 0.004333\n",
      "Epoch 122001, loss: 0.008423\n",
      "Epoch 123001, loss: 0.011064\n",
      "Epoch 124001, loss: 0.005495\n",
      "Epoch 125001, loss: 0.010393\n",
      "Epoch 126001, loss: 0.009262\n",
      "Epoch 127001, loss: 0.005149\n",
      "Epoch 128001, loss: 0.028531\n",
      "Epoch 129001, loss: 0.019836\n",
      "Epoch 130001, loss: 0.024328\n",
      "Epoch 131001, loss: 0.015915\n",
      "Epoch 132001, loss: 0.013302\n",
      "Epoch 133001, loss: 0.022036\n",
      "Epoch 134001, loss: 0.006407\n",
      "Epoch 135001, loss: 0.025736\n",
      "Epoch 136001, loss: 0.026130\n",
      "Epoch 137001, loss: 1.530287\n",
      "Epoch 138001, loss: 4.729657\n",
      "Epoch 139001, loss: 0.030474\n",
      "Epoch 140001, loss: 0.020004\n",
      "Epoch 141001, loss: 0.036813\n",
      "Epoch 142001, loss: 0.031585\n",
      "Epoch 143001, loss: 1.030861\n",
      "Epoch 144001, loss: 0.044756\n",
      "Epoch 145001, loss: 0.019436\n",
      "Epoch 146001, loss: 0.030962\n",
      "Epoch 147001, loss: 0.032822\n",
      "Epoch 148001, loss: 1.060763\n",
      "Epoch 149001, loss: 0.782218\n",
      "Epoch 150001, loss: 0.025278\n",
      "Epoch 151001, loss: 0.023697\n",
      "Epoch 152001, loss: 1.910452\n",
      "Epoch 153001, loss: 0.018443\n",
      "Epoch 154001, loss: 0.016122\n",
      "Epoch 155001, loss: 0.042997\n",
      "Epoch 156001, loss: 0.026197\n",
      "Epoch 157001, loss: 0.009344\n",
      "Epoch 158001, loss: 0.032880\n",
      "Epoch 159001, loss: 0.009717\n",
      "Epoch 160001, loss: 0.014891\n",
      "Epoch 161001, loss: 0.013109\n",
      "Epoch 162001, loss: 0.009120\n",
      "Epoch 163001, loss: 0.008902\n",
      "Epoch 164001, loss: 0.469021\n",
      "Epoch 165001, loss: 0.006114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-425bfb82aeb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnetworkstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetworkaction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetworkreward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \"\"\"\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Must be implemented by subclasses'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"Creates fetch mapper that handles the structure of `fetch`.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "replay_memory = [] # (state, action, reward, terminalstate, state_t+1)\n",
    "epsilon = 0.1\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.9\n",
    "MAX_LEN_REPLAY_MEMORY = 30000\n",
    "FRAMES_TO_PLAY = 300001\n",
    "MIN_FRAMES_FOR_LEARNING = 1000\n",
    "\n",
    "for i_epoch in range(FRAMES_TO_PLAY):\n",
    "    \n",
    "    ### Select an action and perform this\n",
    "    ### EXERCISE: this is where your network should play and try to come as far as possible!\n",
    "    ### You have to implement epsilon-annealing yourself\n",
    "    if random.random() <= epsilon:\n",
    "        action = env.action_space.sample() \n",
    "    else:\n",
    "        pred_q = sess.run(predictedreward, feed_dict={networkstate:[observation]})\n",
    "        action = np.argmax(pred_q)\n",
    "        \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "\n",
    "    ### I prefer that my agent gets 0 reward if it dies\n",
    "    if terminal: \n",
    "        reward = 0\n",
    "        \n",
    "    ### Add the observation to our replay memory\n",
    "    replay_memory.append((observation, action, reward, terminal, newobservation))\n",
    "    \n",
    "    ### Reset the environment if the agent died\n",
    "    if terminal: \n",
    "        newobservation = env.reset()\n",
    "    observation = newobservation\n",
    "    \n",
    "    ### Learn once we have enough frames to start learning\n",
    "    if len(replay_memory) > MIN_FRAMES_FOR_LEARNING: \n",
    "        experiences = random.sample(replay_memory, BATCH_SIZE)\n",
    "        totrain = [] # (state, action, delayed_reward)\n",
    "        \n",
    "        ### Calculate the predicted reward\n",
    "        nextstates = [var[4] for var in experiences]\n",
    "        pred_reward = sess.run(predictedreward, feed_dict={networkstate:nextstates})\n",
    "        \n",
    "        ### Set the \"ground truth\": the value our network has to predict:\n",
    "        for index in range(BATCH_SIZE):\n",
    "            state, action, reward, terminalstate, newstate = experiences[index]\n",
    "            predicted_reward = max(pred_reward[index])\n",
    "            \n",
    "            if terminalstate:\n",
    "                delayedreward = reward\n",
    "            else:\n",
    "                delayedreward = reward + GAMMA*predicted_reward\n",
    "            totrain.append((state, action, delayedreward))\n",
    "            \n",
    "        ### Feed the train batch to the algorithm \n",
    "        states = [var[0] for var in totrain]\n",
    "        actions = [var[1] for var in totrain]\n",
    "        rewards = [var[2] for var in totrain]\n",
    "        _, l, summary = sess.run([optimizer, loss, merged_summary], feed_dict={networkstate:states, networkaction: actions, networkreward: rewards})\n",
    "        \n",
    "\n",
    "        ### If our memory is too big: remove the first element\n",
    "        if len(replay_memory) > MAX_LEN_REPLAY_MEMORY:\n",
    "                replay_memory = replay_memory[1:]\n",
    "\n",
    "        ### Show the progress \n",
    "        if i_epoch%100==1:\n",
    "            summary_writer.add_summary(summary, i_epoch)\n",
    "        if i_epoch%1000==1:\n",
    "            print(\"Epoch %d, loss: %f\" % (i_epoch,l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Testing the algorithm\n",
    "Now we have a trained network that gives use the expected $Q(s,a)$ for a certain state. We can use this to balance the stick (and see how long it lasts) and see what the network predicts at each frame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ResetNeeded",
     "evalue": "Trying to step environment which is currently done. While the monitor is active for CartPole-v0, you cannot step beyond the end of an episode. Call 'env.reset()' to start the next episode.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mResetNeeded\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1ca0b0c61999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mnewobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m### Play till we are dead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/monitoring.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/monitoring.pyc\u001b[0m in \u001b[0;36m_before_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/monitoring/stats_recorder.pyc\u001b[0m in \u001b[0;36mbefore_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trying to step environment which is currently done. While the monitor is active for {}, you cannot step beyond the end of an episode. Call 'env.reset()' to start the next episode.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trying to step an environment before reset. While the monitor is active for {}, you must call 'env.reset()' before taking an initial step.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResetNeeded\u001b[0m: Trying to step environment which is currently done. While the monitor is active for CartPole-v0, you cannot step beyond the end of an episode. Call 'env.reset()' to start the next episode."
     ]
    }
   ],
   "source": [
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "terminal = False\n",
    "while not terminal:\n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    newobservation, reward, terminal, info = env.step(action)\n",
    "\n",
    "### Play till we are dead\n",
    "for _ in range(100):\n",
    "    observation = env.reset()\n",
    "    term = False\n",
    "    predicted_q = []\n",
    "    frames = []\n",
    "    while not term:\n",
    "        rgb_observation = env.render(mode = 'rgb_array')\n",
    "        frames.append(rgb_observation)\n",
    "        pred_q = sess.run(predictedreward, feed_dict={networkstate:[observation]})\n",
    "        predicted_q.append(pred_q)\n",
    "        action = np.argmax(pred_q)\n",
    "        observation, _, term, _ = env.step(action)\n",
    "    print(len(frames))   \n",
    "    \n",
    "    \n",
    "### Plot the replay!\n",
    "#display_frames_as_gif(frames,filename_gif='dqn_run.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-03 12:55:51,134] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/cartpole-experiment-1')\n",
      "[2017-07-03 12:55:51,143] [CartPole-v0] Uploading 1853 episodes of training data\n",
      "[2017-07-03 12:55:53,288] [CartPole-v0] Uploading videos of 11 training episodes (35235 bytes)\n",
      "[2017-07-03 12:55:53,765] [CartPole-v0] Creating evaluation object from /tmp/cartpole-experiment-1 with learning curve and training video\n",
      "[2017-07-03 12:55:54,170] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on CartPole-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_42HU0CptTAWUBmpweA7vbw\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# terminal = False\n",
    "# while not terminal:\n",
    "\n",
    "#     action = env.action_space.sample() \n",
    "#     newobservation, reward, terminal, info = env.step(action)\n",
    "env.close()\n",
    "gym.upload('/tmp/cartpole-experiment-1', api_key='sk_Xpt4s8khRPGveJgB2tUafg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot([var[0] for var in predicted_q])\n",
    "plt.legend(['left', 'right'])\n",
    "plt.xlabel(\"frame\")\n",
    "plt.ylabel('predicted Q(s,a)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Handling difficult situations - team up with your robot\n",
    "You can see in the graph above that our q-function, without the final mistake it made, has a good idea how well it is doing. At moments the pole is going sideways the maximum expected reward lowers. This is a good moment to team up with your robot and guide him when he is in trouble. \n",
    "\n",
    "Collaborating is easy: if your robot does not know what to do, we can ask the user to provide input. The initial state the robot is in gives us a lot of information: $Q(S,a)$ tells us how much reward the robot expects for the next frames of its run. If during execution of the robots strategy the maximum expected $Q$ drops a bit below this number we can interpret this as the robot being in a dire situation. We then ask for the user to say if the cart should move left or right. \n",
    "\n",
    "Note that in the graph above the agent died, even though it expected a lot of reward. This method is not foolproof, but does help the agent to survive longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.ion()\n",
    "observation = env.reset()\n",
    "\n",
    "### We predict the reward for the initial state, if we are slightly below this ideal reward, let the human take over. \n",
    "TRESHOLD = max(max(sess.run(predictedreward, feed_dict={networkstate:[observation]})))-0.2\n",
    "TIME_DELAY = 0.5 # Seconds between frames \n",
    "terminated = False\n",
    "while not terminated:\n",
    "    ### Show the current status\n",
    "    now = env.render(mode = 'rgb_array')\n",
    "    plt.imshow(now)\n",
    "    plt.show()\n",
    "\n",
    "    ### See if our agent thinks it is safe to move on its own\n",
    "    pred_reward = sess.run(predictedreward, feed_dict={networkstate:[observation]})\n",
    "    maxexpected = max(max(pred_reward))\n",
    "    if maxexpected > TRESHOLD: \n",
    "        action = np.argmax(pred_reward)\n",
    "        print(\"Max expected: \" + str(maxexpected))\n",
    "        time.sleep(TIME_DELAY)\n",
    "    else:\n",
    "        ### Not safe: let the user select an action!\n",
    "        action = -1\n",
    "        while action < 0:\n",
    "            try:\n",
    "                action = int(raw_input(\"Max expected: \" + str(maxexpected) + \" left (0) or right(1): \"))\n",
    "                print(\"Performing: \" + str(action))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    ### Perform the action\n",
    "    observation, _, terminated, _ = env.step(action)\n",
    "\n",
    "print(\"Unfortunately, the agent died...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exercises\n",
    "Now that you and your neural network can balance a stick there are many things you can do to improve. As everyones skills are different I wrote down some ideas you can try:\n",
    "#### Machine learning starter: \n",
    "- Improve the neural network. You can toy around with layers (size, type), tune the hyperparameters, or many more. \n",
    "- Toy around with the value of gamma, visualise for several values what kind of behaviour the agent will exercise. Is the agent more careful with a higher gamma value?\n",
    "#### Tensorflow starter: \n",
    "- If you don't have a lot of experience you can either try to improve the neural network, or you can experiment with the Tensorboard tool. Try to add plots of the average reward during training. If you implemented epsilon-greedy exploration this number should go up during training. \n",
    "#### Reinforcement learning starter: \n",
    "- Because our agent only performs random actions our network dies pretty often during training. This means that it has a good idea what to do in its start configurations, but might have a problem when it survived for a longer time. Epsilon-greedy exploration prevents this. With this method you roll a die: with probability epsilon you take a random action, otherwise you take the action the agent thinks is best. You can either set epsilon to a specific value (0.25? 0.1?) or gradually take a lower value to encourage exploration. \n",
    "- Team up with your agent! We already help our agent when he thinks he is in a difficult situation, we could also let it ask for help during training. By letting the agent ask for help with probability epsilon you explore the state space in a way that makes more sense than random exploration, and this will give you a better agent. \n",
    "#### Reinforcement learning itermediate: \n",
    "- Right now we only visualise the loss, which is no indication for how good the network is. According to the paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) the average expected $Q$ should go up during learning (in combination with epsilon-greedy exploration). \n",
    "- Artur Juliani suggests that you can use a [target network](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df). During training your network is very \"unstable\", it \"swings\" in all directions which can take a long time to converge. You can add a second neural network (exactly the same layout as the first one) that calculates the predicted reward. During training, every $X$ frames, you set the weights of your target network equal to the weights of your other network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Conclusion\n",
    "In part two we implemented a deep q-network in Tensorflow, and used it to control a cartpole. We saw that the network can \"know\" when it has problems, and then teamed up with our agent to help him out. Hopefully you enjoyed working with neural networks, the OpenAI gym, and working together with your agent. \n",
    "\n",
    "Initially I wanted to dive into the Atari game environments and skip the CartPole environment for the deep q-networks. Unfortunately, training takes too long (24 hours) before the agent is capable of exercising really cool moves. As I still think it is a lot of fun to learn how to play Atari games I made a third part with some exercises you can take a look at. \n",
    "\n",
    "### Acknowledgments \n",
    "This blogpost is the first part of my TRADR summerschool workshop on using human input in reinforcement learning algorithms. More information can be found [on their homepage](https://sites.google.com/view/tradr/home)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
